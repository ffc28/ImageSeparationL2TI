This is the note (wiki) for the internship of Xhenis 

- An available dataset for old document on line: https://www.isos.dias.ie/english/index.html
- https://scikit-learn.org/stable/index.html
- Separation evaluation tool: https://craffel.github.io/mir_eval/ 


\*May 2020*\

18/05:
Fangchen: 1. I send the convention de stage, some useful links and some documents to Xhenis to start the internship asap. 
	  2. I want to create a place where we can exchange files, codes and figures. If Gitlab.paris13 is not available, then Github is a good tool
	  3. We can start with the document restauration in which some research work are going on. Two things important: a) the mixing model, pay attention to the fact that the pixels are positive. b) Independent Assumption: I don't know if the image sources recto verso are independent or not
	  4. We can begin with a "Fake" mixing model (mixing of one block of images without mean value) to test the independent assumption
	  5. Then we'll do something with the mixing model. Maybe this problem is easy
	  6. Why are the two source components correlated? A decomposition into a specific dictionary can reduce the independency? This can also include the learning process for Xhenis
	  7. For the desmoking problem, the first idea is to put it into the frame of multichannel separation: several blocks or several frames of video with hypothesis that the smoking is varying slowly.

26/05:
Fangchen: I had a reunion with Xhenis with some questions in the paper: blind separation for document restauration (eq 5.2 and the source superposition factor):
	  1. We talked about the source separation (BSS) in general. The BSS is usually treated in two scenarios, determined and underdetermined. In determined case, we use ICA, in underdetermined case, we use sparsity.
	  2. We talked about the zero-mean problem: why do we usually remove the mean value of the observation before the separation? (ambiguity problem? weighting problem?) Is it suitable for images? What does the mean value of an image represent? Is it important?
	  3. Even the negative value of an image pixel does not make sense, it does not block us from using negative values during the separation. But the authors of the paper think differently. Why?
	  4. We talked about the constrains on the mixing matrix. Normally we have a normalisation constrain on the column of the mixing matrix. But in the paper, they use a sum-row-to-one constrain. Why? Does it have sth to do with the background?
	  5. We talked about the ambiguity problem of the separation. The permutation ambiguity is harmful when we deal with several separations (several blocks). Xhenis thinks that using superposed blocks can help. How does this paper do it?
	  6. I think that the mathematical formulation in section 5 is there because they want to keep the images positive all the time and they have this sum-row-to-one constrain. Is it really necessary?
	  7. We talked about the whitening pre-processing. Same question: is it suitable for images?
 	  8. Xhenis thinks that the sum-row-to-one constrain on the mixing matrix can be linked to probability. It's a very interesting interpretaton. How can it help us see things more clearly?
	
27/05:
Fangchen: I had a reunion with Xhenis where we discussed about the paper blind separation for document restauration
	  1. The sum-row-to-one constrain of the mixing matrix is just to satisfy the interval of the mixtures and the sources. However, it can not remove the scaling ambiguity
          2. My guess why they use this positivity constrain of the sources is due to the non-linear mixing model. We've to find way to verify if the local linear model is correct for zero-mean sources
	  3. We want to investigate the correlation condition of the real sources. Xhenis thinks that the real sources could be naturally correlated (not just because they're not zero-mean)
	  4. For image containing documents, do we have the saturation problem? I guess it's not like audio signals

29/05:
Fangchen: 1. With Xhenis, we look at the BSS with images using sparsity. We looked at the Laplacian and dictionary learning technique to make images sparse
	  2. I suggest that, in a first time, Xhenis can use some source images (ground truth) to synthesis the mixtures using traditional model (zero mean, etc). Then apply the sparsity-based (Laplacian or Wavelet) method to separate the source
	  3. Then in a second step, we will investigate the real-world model of document bleed-through and find a more adapted mixing model




\*Juin 2020*\

02/06:
Fangchen: 1. One possible idea: for now the document separation is based on ICA or dictionary learning based image inpainting. I think we can combine them and use dictionary-based BSS for determined separation.
	  2. This problem is already investigated in Abolghasemi, V., Ferdowsi, S., & Sanei, S. (2012). Blind separation of image sources via adaptive dictionary learning. IEEE Transactions on Image Processing, 21(6), 2921-2930. Then our contribution can be the convolutional (2D) model
	  3. The underdetermined BSS is another possible track


08/06:
Fangchen: 1. With Xhenis, we talked about the BCD (PALM) algorithm and the dictionary learning algorithm.
          2. We can use the ICA measurement to measure the independence level of old documents which is a big motivation of using sparsity-based method.
	  3. We can compare the naive (sparsity in spatial domain of images) sparse method with ICA to see the difference
   	  4. We can start to use dictionary learning package to learn some toy dictionary and to see if the implementation is realizable or not.
     	  5. We talked about the Bayesian interpretation of the l2-l1 minimisation problem
	  6. We've to provide some quantitative measures for the evaluations (SDR, SIR and SAR). In the worst case, I translate the Matlab package to Python
	  7. Xhenis should have access to the calculation cluster

10/06:
Azeddine: 1. For synthesis mixtures, the evaluations can be done with the help of ground truth images, but in real-world applications, the ground truth is not available.
          2. Instead of evaluating in a pixel-wise way the separated images, we should think of the statistical way. The idea is to compare the statistical distribution of the clean image and the separated image
	  3. Xhenis can start in a parallel way of studying the image separation and image evaluation.
  	  4. Azeddine also talked about the multi-scale thing

16/06
Fangchen: 1. In FastICA, instead of using the logcosh as the non-linearity to measure the nongaussianity, the function cube is a better idea as it suits better the sparse source (sources with super-Gaussian distribution)
          2. The image pyramid decomposition is a good way to deal with the mean value of the patches.


17/06
Azeddine: 1. Xhenis should make a plan of the internship as a function of the soutenance date etc
 	  2. Measuring the quality of an image is different from measuring the separation performance. We should concentrate on the similarity between the original source images and the estimated one
          3. Xhenis'll prepare a presentation (some pages of slides) to present what she does during the week. This can be done once two weeks. This will help Xhenis to better organise the internship
	  4. For ICA, we should start with simple examples (some source images that we know are independent) to see the efficiency of the FastICA or ICA method. 
	  5. At the same time, we can try to use the block methods to test the FastICA. Dividing the images into block is a good way to break the dependency between the images so that we can vectorize them.


18/06
Fangchen: 1. Xhenis starts using the package of randomized dependence coefficient to measure the independence between two signals (1d or 2d)
	  2. In the paper of "A blind source separation technique for document restoration", the author suppose that the source images are of positive values and they don't remove the mean during the separation.
   	  3. I thinks that it does not violate our mixing model where we remove the mean before the separation (because we suppose that the sources are of zero mean).
	  4. Maybe we can do something like this: we first learn the dictionary from some clean document images to get the dictionary. Then we assume a linear mixing model of document images in a small block (the same block used for dictionary learning). Then we do the separation for each block with their own mixing matrix based on the learned dictionary
	  5. This idea has a huge link with IVA: in IVA we have a linear separation problem for each frequency in the STFT domain. Here we have a linear separation problem for each column in the code domain. We can try to use Group-Lasso or WGL to introduce some correlation between blocks because we use overlapped blocks here. 
  	  6. I'm not sure if we should use the same block in the dictionary learning part and the separation part. But why not? 
     	  7. Find a way to add the pyramid decomposition. It's still unclear to me.
	  8. We're now not exploiting any special properties of document images. Maybe we should stay in this way so that we can include medical images?
	 
24/06
Azeddine: 1. For the plan of Xhenis, it does not mean that we should not do any investigation during the experimenting phase. This should be flexible
	  2. Xhenis send the link of the timeline to everyone.
  	  3. Fangchen should send to everyone a plan of the day before the meeting
	  4. The package RDC is maybe for random signal. Does it mean anything for determinist signals? We should know how it functions before using it.
	  5. We don't usually change the sampling rate. When we want to change the number of samples, we change the scale of the signal.
	  6. Xhenis can try the following: start with one signal and then get another dependent signal and see the performance of ICA.

Xhenis:   1. Xhenis did a presentation where she shows what she did in the last few weeks
   	  2. Xhenis reported a question of using the package of evaluation (SDR) and shows the problem of the SDR when the number of samples is changed when the sources signals are somehow correlated.
	  3. Fangchen thinks it may due to the permutation problem.
	  4. Fangchen suggests that Xhenis should first evaluation the mixtures to get a baseline of SDR and then see how FastICA improves the SDR.
	  5. Xhenis continues to do the image separation with packages and pay attention to the permutation and scaling ambiguity problem.

30/06
Fangchen: 1. The first experiment shows that diving the images into patches is not a bad idea for separation: the separation performance (SDR) with the whole image is less than the patch method.
	  2. In order to make sure of all this, we have to try with different mixing matrix, different document images, different non-linearity and eventually different method
 	  3. I can code the sparsity-based method and compare it with FastICA for with/without patch settings
	  4. The permutation alignment is important: for now we're using oracle permutation. In practice, we can either use the estimated mixing matrix or the estimated sources to perform the permutation
	  5. The scaling ambiguity could be difficult. But I think it can be solved based on some assumptions of the mixing matrix (e.g. the first element of the mixing matrix should be 1, or the sum of each column is 1, to verify) 
	  6. For now, we're using instantaneous mixing process. Later on, we can try with non-linear mixing process (paper: Low quality document image modeling and enhancement). For me, the patch method could be much better in this setting.
	  7. I don't think there're other papers about this patch issue for separation problem. To verify!

	

\*July 2020*\
02/07
Fangchen: 1. Xhenis has some results of the comparison between the patch method and no-patch method for separation. She puts all the results on the wiki page.
	  2. With these results, I think we can now try the sparsity-based method and/or dictionary learning based method
 	  3. For now, the multi-scale is not put into use. Maybe it can help us with the scaling ambiguity?
	  4. The permutation can be maybe solved with the assumptions on the mixing matrix


09/07
Fangchen: 1. Xhenis's results show that patch method depend on the image used. I think patch method could lead to better separation if the source images are somehow correlated (high RDC). The patch method relax the mixing matrix for each patch and plays with the non-stationarity of images. But I think it's still random, and the results depend on the images used.
	  2. RDC is suitable for random signals like images or audio, not determinist signals
	  3. The patch methods should be much better if we're considering non-stationary and non-linear mixing model. I'll take a look and try to code the two models
	  4. But we have to know why the non-stationary and non-linear mixing model can be linearly approximated in a small region.
	  5. The non-stationarity of images can be dealt with the DCT or dictionary-based methods in which small patch is used. But at the same time the mixing matrix is not relaxed.
	  6. The advantages of sparsity and dictionary-based methods is that they can deal with correlated images.
	  7. Maybe we can stick to the linear model for synthesized mixtures and patch method for real-world images?


15/07:
Azeddine: 1. The figure that Xhenis has presented has limited number of points: more experiments should be added. It might not be the best way to link all the points together
	  2. When we try with natural images (document images), we don't know the statistical characteristics about the images. We might want to precede in a controlled manner
	  3. Eventually, we should consider color images. Color images can potentially help the separation as it is linked to diversity.
	  4. Xhenis synthesis a report before next Monday.  

Fangchen: 1. It would be interesting to compare the RDC before and after the transform (DCT or dictionary)


16/07:
Fangchen: 1. It would be interesting to compare the sparsity level vs the correlation level of the estimated images
	  2. It seems that the plug-in methods work for determined case for image sources. But why? In theory, we can use all the denoising methods that we want
	  3. It seems that the plug-in method depend on the condition number of the mixing matrix. Maybe it's a good idea to do the whitening processing. But if we do that, can we still deal with the partially correlated sources?
	  4. Are there "learning"-based denoising methods?
	  5. One of the advantages of plug-in is maybe to deal with the color images using the color images denoising methods.
	  6. If plug-in methods work, then we need to concentrate on the mixing model. The linear stationary model is too simple. 
	  7. We should of course compare the plug-in dictionary method with the analysis and synthesis dictionary method




\*August 2020*\

\*Septembre 2020*\

\*Octobre 2020*\

\*Novembre 2020*\

\*December 2020*\

