This is the note (wiki) for the internship of Xhenis 

\*May 2020*\

18/05:
Fangchen: 1. I send the convention de stage, some useful links and some documents to Xhenis to start the internship asap. 
	  2. I want to creat a place where we can exchange files, codes and figures. If Gitlab.paris13 is not avaible, then Github is a good tool
	  3. We can start with the document restauration in which some research work are going on. Two things important: a) the mixing model, pay attention to the fact that the pixels are positive. b) Independent Assumption: I don't kown if the image sources recto verso are independent or not
	  4. We can begin with a "Fake" mixing model (mixing of one block of images without mean value) to test the independent assumption
	  5. Then we'll do something with the mixing model. Maybe this problem is easy
	  6. Why are the two source components correlated? A decomposition into a specific dictionary can reduce the indenpendency? This can also include the learning process for Xhenis
	  7. For the desmoking problem, the first idea is to put it into the frame of multichannel separation: several blocks or several frames of video with hypothesis that the smoking is varing slowly.
26/05:
Fangchen: I had a reunion with Xhenis with some questions in the paper: blind separation for document restauration (eq 5.2 and the source superposition factor):
	  1. We talked about the source separation (bss) in general. The bss is usually treated in two scenarios, determined and underdetermined. In determined case, we use ICA, in underdetermined case, we use sparsity.
	  2. We talked about the zero-mean problem: why do we usually remove the mean value of the observation before the separation? (ambiguity problem? weighting problem?) Is it suitable for images? What does the mean value of an image represent? Is it important?
	  3. Even the negative value of an image pixel does not make sense, it does not block us from using negative values during the separation. But the authors of the paper think differently. Why?
	  4. We talked about the constrains on the mixing matrix. Normally we have a normalisation constrain on the column of the mixing matrix. But in the paper, they use a sum-row-to-one constrain. Why? Does it have sth to do with the background?
	  5. We talked about the ambiguity problem of the separation. The permutation ambiguity is harmful when we deal with several separations (several blocks). Xhenis thinks that using superposed blocks can help. How does this paper do it?
	  6. I think that the mathematical formuation in section 5 is there because they want to keep the images positive all the time and they have this sum-row-to-one constrain. Is it really necessary?
	  7. We talked about the whitening pre-processing. Same question: is it suitable for images?
 	  8. Xhenis thinks that the sum-row-to-one constrain on the mixing matrix can be linked to probability. It's a very interesting interpretaton. How can it help us see things more clearly?
	
27/05:
Fangchen: I had a reunion with Xhenis where we discussed about the paper blind separation for document restauration
	  1. The sum-row-to-one constrain of the mixing matrix is just to satisfie the interval of the mixtures and the sources. However, it can not remove the scaling ambiguity
          2. My guess why they use this positivity constrain of the sources is due to the non-linear mixing model. We've to find way to verify if the local linear model is correct for zero-mean sources
	  3. We want to investigate the correlation condition of the real sources. Xhenis thinks that the real sources could be naturally correlated (not just because they're not zero-mean)





\*Juin 2020*\

\*July 2020*\

\*August 2020*\

\*Septembre 2020*\

\*Octobre 2020*\

