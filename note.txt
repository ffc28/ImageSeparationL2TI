This is the note (wiki) for the internship of Xhenis 

- An available dataset for old document on line: https://www.isos.dias.ie/english/index.html
- https://scikit-learn.org/stable/index.html


\*May 2020*\

18/05:
Fangchen: 1. I send the convention de stage, some useful links and some documents to Xhenis to start the internship asap. 
	  2. I want to creat a place where we can exchange files, codes and figures. If Gitlab.paris13 is not avaible, then Github is a good tool
	  3. We can start with the document restauration in which some research work are going on. Two things important: a) the mixing model, pay attention to the fact that the pixels are positive. b) Independent Assumption: I don't kown if the image sources recto verso are independent or not
	  4. We can begin with a "Fake" mixing model (mixing of one block of images without mean value) to test the independent assumption
	  5. Then we'll do something with the mixing model. Maybe this problem is easy
	  6. Why are the two source components correlated? A decomposition into a specific dictionary can reduce the indenpendency? This can also include the learning process for Xhenis
	  7. For the desmoking problem, the first idea is to put it into the frame of multichannel separation: several blocks or several frames of video with hypothesis that the smoking is varing slowly.

26/05:
Fangchen: I had a reunion with Xhenis with some questions in the paper: blind separation for document restauration (eq 5.2 and the source superposition factor):
	  1. We talked about the source separation (bss) in general. The bss is usually treated in two scenarios, determined and underdetermined. In determined case, we use ICA, in underdetermined case, we use sparsity.
	  2. We talked about the zero-mean problem: why do we usually remove the mean value of the observation before the separation? (ambiguity problem? weighting problem?) Is it suitable for images? What does the mean value of an image represent? Is it important?
	  3. Even the negative value of an image pixel does not make sense, it does not block us from using negative values during the separation. But the authors of the paper think differently. Why?
	  4. We talked about the constrains on the mixing matrix. Normally we have a normalisation constrain on the column of the mixing matrix. But in the paper, they use a sum-row-to-one constrain. Why? Does it have sth to do with the background?
	  5. We talked about the ambiguity problem of the separation. The permutation ambiguity is harmful when we deal with several separations (several blocks). Xhenis thinks that using superposed blocks can help. How does this paper do it?
	  6. I think that the mathematical formuation in section 5 is there because they want to keep the images positive all the time and they have this sum-row-to-one constrain. Is it really necessary?
	  7. We talked about the whitening pre-processing. Same question: is it suitable for images?
 	  8. Xhenis thinks that the sum-row-to-one constrain on the mixing matrix can be linked to probability. It's a very interesting interpretaton. How can it help us see things more clearly?
	
27/05:
Fangchen: I had a reunion with Xhenis where we discussed about the paper blind separation for document restauration
	  1. The sum-row-to-one constrain of the mixing matrix is just to satisfie the interval of the mixtures and the sources. However, it can not remove the scaling ambiguity
          2. My guess why they use this positivity constrain of the sources is due to the non-linear mixing model. We've to find way to verify if the local linear model is correct for zero-mean sources
	  3. We want to investigate the correlation condition of the real sources. Xhenis thinks that the real sources could be naturally correlated (not just because they're not zero-mean)
	  4. For image containing documents, do we have the saturation problem? I guess it's not like audio signals

29/05:
Fangchen: 1. With Xhenis, we look at the bss with images using sparsity. We looked at the Laplacian and dictionary learnig technique to make images sparse
	  2. I suggest that, in a first time, Xhenis can use some source images (ground truth) to synthesis the mixtures using traditional model (zero mean, etc). Then apply the sparsity-based (Laplacian or Wavelet) method to separate the source
	  3. Then in a second step, we will investigate the real-world model of document bleed-through and find a more adapted mxing model


\*Juin 2020*\

02/06:
Fangchen: 1. One possible idea: for now the document separation is based on ICA or dictionary learning based image inpaiting. I think we can combine them and use dictionary-based BSS for determined separation.
	  2. This problem is already investigated in Abolghasemi, V., Ferdowsi, S., & Sanei, S. (2012). Blind separation of image sources via adaptive dictionary learning. IEEE Transactions on Image Processing, 21(6), 2921-2930. Then our contribution can be the convolutive (2D) model
	  3. The underdetermined BSS is another possible track


08/06:
Fangchen: 1. With Xhenis, we talked about the BCD (PALM) algorithm and the dictionary learning algorithm.
          2. We can use the ICA measurement to measure the independence level of old documents which is a big motivation of using sparsity-based method.
	  3. We can compare the naive (sparsity in spatial domain of images) sparse method with ICA to see the difference
   	  4. We can start to use dictionary learning pack to learn some toy dictionary and to see if the implementation is realisable or not.
     	  5. We talked about the Bayesian interpretation of the l2-l1 minimisation problem
	  6. We've to provide some quantitative measures for the evaluations (SDR, SIR and SAR). In the worst case, I translate the Matlab package to Python
	  7. Xhenis should have access to the calulation cluster



\*July 2020*\

\*August 2020*\

\*Septembre 2020*\

\*Octobre 2020*\

\*Novembre 2020*\

\*December 2020*\

